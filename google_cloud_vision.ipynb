{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Video classification using Google Cloud Vision API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "Text\n",
      "Font\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\n",
    "    'GOOGLE_APPLICATION_CREDENTIALS'] = \"C:/Users/hites/credentials.json\"\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# The name of the image file to annotate\n",
    "file_name = os.path.abspath('C:/Users/hites/image.jpg')\n",
    "\n",
    "# Loads the image into memory\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = types.Image(content=content)\n",
    "\n",
    "# Performs label detection on the image file\n",
    "response = client.label_detection(image=image)\n",
    "labels = response.label_annotations\n",
    "\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we extract the image from URL and will check for labels and respected confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (and confidence score):\n",
      "==============================\n",
      "People (95.05%)\n",
      "Street (89.12%)\n",
      "Mode of transport (89.09%)\n",
      "Transport (85.13%)\n",
      "Vehicle (84.69%)\n",
      "Snapshot (84.11%)\n",
      "Urban area (80.29%)\n",
      "Infrastructure (73.14%)\n",
      "Road (72.74%)\n",
      "Pedestrian (68.90%)\n"
     ]
    }
   ],
   "source": [
    "image_uri = 'gs://cloud-samples-data/vision/using_curl/shanghai.jpeg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.types.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.label_detection(image=image)\n",
    "\n",
    "print('Labels (and confidence score):')\n",
    "print('=' * 30)\n",
    "for label in response.label_annotations:\n",
    "    print(label.description, '(%.2f%%)' % (label.score*100.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "CAUTION\n",
      "Otters crossing\n",
      "for next 6 miles\n",
      "\n",
      "bounds: (61,243),(251,243),(251,340),(61,340)\n",
      "==============================\n",
      "CAUTION\n",
      "bounds: (75,245),(235,243),(235,269),(75,271)\n",
      "==============================\n",
      "Otters\n",
      "bounds: (65,296),(140,297),(140,315),(65,314)\n",
      "==============================\n",
      "crossing\n",
      "bounds: (151,295),(247,297),(247,318),(151,316)\n",
      "==============================\n",
      "for\n",
      "bounds: (61,322),(94,322),(94,340),(61,340)\n",
      "==============================\n",
      "next\n",
      "bounds: (106,322),(156,322),(156,340),(106,340)\n",
      "==============================\n",
      "6\n",
      "bounds: (167,321),(180,321),(180,339),(167,339)\n",
      "==============================\n",
      "miles\n",
      "bounds: (191,321),(251,321),(251,339),(191,339)\n"
     ]
    }
   ],
   "source": [
    "image_uri = 'gs://cloud-vision-codelab/otter_crossing.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.types.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print('=' * 30)\n",
    "    print(text.description)\n",
    "    vertices = ['(%s,%s)' % (v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    print('bounds:', \",\".join(vertices))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the description and score of Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "mid: \"/g/120xtw6z\"\n",
      "description: \"Trocad\\303\\251ro Gardens\"\n",
      "score: 0.9097840785980225\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 321\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 54\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 371\n",
      "  }\n",
      "  vertices {\n",
      "    x: 321\n",
      "    y: 371\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.861596299999995\n",
      "    longitude: 2.2892823\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.5950314998626709\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 401\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 72\n",
      "  }\n",
      "  vertices {\n",
      "    x: 513\n",
      "    y: 353\n",
      "  }\n",
      "  vertices {\n",
      "    x: 401\n",
      "    y: 353\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.858461\n",
      "    longitude: 2.294351\n",
      "  }\n",
      "}\n",
      "\n",
      "==============================\n",
      "mid: \"/m/02j81\"\n",
      "description: \"Eiffel Tower\"\n",
      "score: 0.4474566876888275\n",
      "bounding_poly {\n",
      "  vertices {\n",
      "    x: 25\n",
      "    y: 216\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 216\n",
      "  }\n",
      "  vertices {\n",
      "    x: 531\n",
      "    y: 374\n",
      "  }\n",
      "  vertices {\n",
      "    x: 25\n",
      "    y: 374\n",
      "  }\n",
      "}\n",
      "locations {\n",
      "  lat_lng {\n",
      "    latitude: 48.875072\n",
      "    longitude: 2.312622\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_uri = 'gs://cloud-vision-codelab/eiffel_tower.jpg'\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.types.Image()\n",
    "image.source.image_uri = image_uri\n",
    "\n",
    "response = client.landmark_detection(image=image)\n",
    "\n",
    "for landmark in response.landmark_annotations:\n",
    "    print('=' * 30)\n",
    "    print(landmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we will classify the images according to the sentiments present in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "File: face_surprise.jpg\n",
      "Face surprised: LIKELY\n",
      "Face bounds: (93,425),(520,425),(520,922),(93,922)\n",
      "==============================\n",
      "File: face_no_surprise.png\n",
      "Face surprised: VERY_UNLIKELY\n",
      "Face bounds: (120,0),(334,0),(334,198),(120,198)\n"
     ]
    }
   ],
   "source": [
    "uri_base = 'gs://cloud-vision-codelab'\n",
    "pics = ('face_surprise.jpg', 'face_no_surprise.png')\n",
    "\n",
    "client = vision.ImageAnnotatorClient()\n",
    "image = vision.types.Image()\n",
    "\n",
    "for pic in pics:\n",
    "    image.source.image_uri = '%s/%s' % (uri_base, pic)\n",
    "    response = client.face_detection(image=image)\n",
    "\n",
    "    print('=' * 30)\n",
    "    print('File:', pic)\n",
    "    for face in response.face_annotations:\n",
    "        likelihood = vision.enums.Likelihood(face.surprise_likelihood)\n",
    "        vertices = ['(%s,%s)' % (v.x, v.y) for v in face.bounding_poly.vertices]\n",
    "        print('Face surprised:', likelihood.name)\n",
    "        print('Face bounds:', \",\".join(vertices))"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio transcripts into text ...\n",
      "birch canoes on the smooth thanks to the dark blue background it is easy to tell the depth of a well these days are there this rice is often served in rambles the juice of lemons makes fine punch the box was done beside the Portrack the hogs are search Up Korn and garbage\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer class (for recognizing the speech)\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Reading Audio file as source\n",
    "# listening the audio file and store in audio_text variable\n",
    "\n",
    "with sr.AudioFile('testaudio.wav') as source:\n",
    "    \n",
    "    audio_text = r.listen(source)\n",
    "    \n",
    "# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "    try:\n",
    "        \n",
    "        # using google speech recognition\n",
    "        text = r.recognize_google(audio_text)\n",
    "        print('Converting audio transcripts into text ...')\n",
    "        print(text)\n",
    "     \n",
    "    except:\n",
    "         print('Sorry.. run again...')"
   ]
  },

 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
